# Binocular Balance

![Generated by DALLÂ·E](https://raw.githubusercontent.com/Jim137/bbalance/main/doc/fig/head.png)

This is the final project for the course, "Introduction to Neurophysics" in National Tsing Hua University. 
The goal of this project is to study the binocular balance.

## Introduction

Neural networks, the intricate webs of neurons in our brains, are not just conduits for electrical impulses but the very foundation of learning and perception. 
Among the various mechanisms governing their adaptability, Hebbian learning stands out as a pivotal concept.

In the followings, we will briefly introduce some basic concepts of Hebbian learning, BCM theory and binocular balance.

### 1. Hebbian Learning

[Hebbian learning](https://en.wikipedia.org/wiki/Hebbian_theory), a fundamental concept in neuroscience, is named after Donald Hebb who proposed it in his 1949 book "The Organization of Behavior." 
It's often summarized by the phrase "**neurons that fire together, wire together**." 
This principle suggests that synaptic connections between neurons are strengthened when they are activated simultaneously. 
Hebbian learning is crucial for understanding how experiences and behaviors can lead to changes in the brain's neural networks. 
It's a form of synaptic plasticity, playing a key role in learning and memory. 
This concept has been instrumental in the development of theories about neural network function and is a foundational element in various fields, including computational neuroscience and psychology.

Consider $i$-th presynaptic neuron and $j$-th postsynaptic neuron.
The synaptic weight $w_{ji}$ can be described as following equation:

$$
w_{ji}(t+1) = w_{ji}(t) + \gamma y_j(t) x_{i}(t)
$$

Or, in continuous limit:

$$
\tau_w \frac{dw_{ji}}{dt} = y_j x_i
$$

where $\tau_w$ is the time constant of the synaptic weight, $y_j$ is the postsynaptic activity, and $x_i$ is the presynaptic activity.

Therefore, we can write down the the dynamics for neurons' activity as following:

$$
\tau \frac{dy_j}{dt} = -y_j + G\left(\sum_i w_{ji} x_i\right)
$$

where $\tau$ is the time constant of the neuronal activity, and $G$ is the gain function.

From experiments, we know that the dynamics of neuronal activity is faster than the dynamics of synaptic weight, which is $\tau \ll \tau_w$.
And we will have the following approximation (Steady-state approximation):

$$
y_j = G\left(\sum_i w_{ji} x_i\right)
$$

Then substitute $y_j$ into the equation of synaptic weight, we will have:

$$
\tau_w \frac{dw_{ji}}{dt} = G\left(\sum_k w_{jk} x_k\right) x_i
$$

Note that $k$ is dummy index.

We can see that if $x_i$ dominate the sum and hit the threshold of the gain function $G$, the synaptic weight will be increased.

However, the synaptic weight will diverge if there is no inhibition.
Consequently, we need to introduce the [Oja's rule](https://en.wikipedia.org/wiki/Oja%27s_rule) to renormalize the synaptic weight.
The oja-modified Hebbian learning is:

$$
\tau_w \frac{dw_{ji}}{dt} = y_j x_i - y_j^2 w_{ji}
$$

And the discrete version is:

$$
w_{ji}(t+1) = w_{ji}(t) + \eta \left[G\left(\sum_k w_{jk}(t) x_k\right)x_i-G\left(\sum_k w_{jk}(t) x_k\right)^2w_{ji}(t)\right]
$$

where $\eta$ is the learning rate.
And which is the iteration what we will use in the following simulations.

### 2. BCM Theory



### 3. Binocular Balance



## Methodology



## Results



## Conclusion



## Reference



## Appendix


